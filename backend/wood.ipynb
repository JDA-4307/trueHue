{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1c5a1b2e-3f94-4a7a-9b1e-1234567890ab",
      "metadata": {},
      "source": [
        "# Wood Finish Classification using LAB Color Space\n",
        "\n",
        "This notebook builds a classification model for wood finishes (medium cherry, desert oak, and graphite walnut) by preprocessing images to LAB color space for improved accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a9e4b7f-6d72-4e6b-a8e4-0987654321cd",
      "metadata": {},
      "source": ["## 1. Setup and Imports"]
    },
    {
      "cell_type": "code",
      "id": "3f4c7e2d-8d9f-4b51-90d2-abcdef123456",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# For image processing\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# For model building\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# For evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# Check if GPU is available\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available: \", len(tf.config.list_physical_devices('GPU')) > 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d9a7c3b-1f6e-4f8a-82bd-112233445566",
      "metadata": {},
      "source": [
        "## 2. Data Loading and LAB Preprocessing\n",
        "\n",
        "### 2.1 Configure Data Paths"
      ]
    },
    {
      "cell_type": "code",
      "id": "5e8f2a0c-0a3d-4d61-bc3f-66778899aabb",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define constants\n",
        "IMG_SIZE = 224  # Input size expected by most pretrained models\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 3\n",
        "CLASS_NAMES = [\"medium_cherry\", \"desert_oak\", \"graphite_walnut\"]\n",
        "\n",
        "# Set your data directory path here\n",
        "BASE_DATA_DIR = Path(\"./wood_finishes_dataset\")  # Update this to your dataset path\n",
        "\n",
        "# Make sure the paths exist\n",
        "for class_name in CLASS_NAMES:\n",
        "    class_path = BASE_DATA_DIR / class_name\n",
        "    if not class_path.exists():\n",
        "        print(f\"Warning: Directory {class_path} does not exist\")\n",
        "    else:\n",
        "        num_images = len(list(class_path.glob('*.jpg')) + list(class_path.glob('*.png')))\n",
        "        print(f\"Found {num_images} images in {class_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a7b8c9d-0e1f-4a2b-90c1-d23456789abc",
      "metadata": {},
      "source": ["### 2.2 Load and Convert Images to LAB Color Space"]
    },
    {
      "cell_type": "code",
      "id": "7d8e9f10-1121-3141-5161-7181920a0b0c",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_convert_to_lab(base_dir, class_names, img_size):\n",
        "    \"\"\"\n",
        "    Load images from directories and convert to LAB color space\n",
        "    \n",
        "    Args:\n",
        "        base_dir: Base directory containing class folders\n",
        "        class_names: List of class folder names\n",
        "        img_size: Target size for images (img_size x img_size)\n",
        "        \n",
        "    Returns:\n",
        "        images: Numpy array of LAB images\n",
        "        labels: Numpy array of integer labels\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "    base_dir = Path(base_dir)\n",
        "    \n",
        "    for i, class_name in enumerate(class_names):\n",
        "        class_dir = base_dir / class_name\n",
        "        print(f\"Processing class {i}: {class_name}\")\n",
        "        \n",
        "        # Check if directory exists\n",
        "        if not class_dir.exists():\n",
        "            print(f\"Warning: Directory {class_dir} not found. Skipping.\")\n",
        "            continue\n",
        "        \n",
        "        # Get all image files\n",
        "        image_files = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.jpeg')) + \\\n",
        "                      list(class_dir.glob('*.png')) + list(class_dir.glob('*.bmp'))\n",
        "        \n",
        "        print(f\"Found {len(image_files)} images in {class_name}\")\n",
        "        \n",
        "        for img_path in tqdm(image_files, desc=class_name):\n",
        "            try:\n",
        "                # Read image using OpenCV\n",
        "                img = cv2.imread(str(img_path))\n",
        "                if img is None:\n",
        "                    print(f\"Warning: Could not read {img_path}. Skipping.\")\n",
        "                    continue\n",
        "                \n",
        "                # Convert BGR to RGB (OpenCV loads as BGR)\n",
        "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                \n",
        "                # Resize to target dimensions\n",
        "                img_resized = cv2.resize(img_rgb, (img_size, img_size))\n",
        "                \n",
        "                # Convert RGB to LAB\n",
        "                img_lab = cv2.cvtColor(img_resized, cv2.COLOR_RGB2LAB)\n",
        "                \n",
        "                # Normalize LAB values to 0-1 range for the model\n",
        "                # L channel is in range [0, 100], a and b channels are in range [-127, 127]\n",
        "                l_channel = img_lab[:,:,0] / 100.0  # Normalize L to [0, 1]\n",
        "                a_channel = (img_lab[:,:,1] + 127) / 255.0  # Normalize a to [0, 1]\n",
        "                b_channel = (img_lab[:,:,2] + 127) / 255.0  # Normalize b to [0, 1]\n",
        "                \n",
        "                # Stack normalized channels\n",
        "                normalized_lab = np.stack([l_channel, a_channel, b_channel], axis=-1)\n",
        "                \n",
        "                images.append(normalized_lab)\n",
        "                labels.append(i)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {img_path}: {e}\")\n",
        "    \n",
        "    if len(images) == 0:\n",
        "        raise ValueError(\"No images were loaded. Please check the directory structure and image formats.\")\n",
        "    \n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load all images and convert to LAB\n",
        "print(\"Loading images and converting to LAB color space...\")\n",
        "all_images, all_labels = load_and_convert_to_lab(BASE_DATA_DIR, CLASS_NAMES, IMG_SIZE)\n",
        "print(f\"Loaded {len(all_images)} total images with shape {all_images.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b4d5e6f-7a8b-9c0d-1e2f-34567890abcd",
      "metadata": {},
      "source": [
        "### 2.3 Visualize LAB Color Space\n",
        "\n",
        "Let's visualize the LAB color space to understand what our model will be working with:"
      ]
    },
    {
      "cell_type": "code",
      "id": "9a0b1c2d-3e4f-5061-7283-abcdef098765",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_lab_channels(images, labels, class_names, num_samples=2):\n",
        "    \"\"\"\n",
        "    Visualize LAB channels for sample images from each class\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 5 * len(class_names)))\n",
        "    \n",
        "    for i, class_name in enumerate(class_names):\n",
        "        # Find indices of images from this class\n",
        "        indices = np.where(labels == i)[0]\n",
        "        \n",
        "        # Randomly sample some images\n",
        "        if len(indices) > 0:\n",
        "            sample_indices = np.random.choice(indices, \n",
        "                                             size=min(num_samples, len(indices)), \n",
        "                                             replace=False)\n",
        "            \n",
        "            for j, idx in enumerate(sample_indices):\n",
        "                lab_img = images[idx]\n",
        "                \n",
        "                # Extract LAB channels\n",
        "                l_channel = lab_img[:,:,0]  # Already normalized to [0, 1]\n",
        "                a_channel = lab_img[:,:,1]  # Already normalized to [0, 1]\n",
        "                b_channel = lab_img[:,:,2]  # Already normalized to [0, 1]\n",
        "                \n",
        "                # Convert back to RGB for display\n",
        "                # First denormalize\n",
        "                l_denorm = l_channel * 100\n",
        "                a_denorm = a_channel * 255 - 127\n",
        "                b_denorm = b_channel * 255 - 127\n",
        "                # Reconstruct LAB image (OpenCV format)\n",
        "                lab_denorm = np.stack([l_denorm, a_denorm, b_denorm], axis=-1).astype(np.uint8)\n",
        "                # Convert back to RGB\n",
        "                rgb_img = cv2.cvtColor(lab_denorm, cv2.COLOR_LAB2RGB)\n",
        "                \n",
        "                # Plot images\n",
        "                row_base = i * num_samples + j\n",
        "                \n",
        "                # Original (converted back to RGB)\n",
        "                plt.subplot(len(class_names) * num_samples, 4, row_base * 4 + 1)\n",
        "                plt.imshow(rgb_img)\n",
        "                plt.title(f\"{class_name} (RGB View)\")\n",
        "                plt.axis('off')\n",
        "                \n",
        "                # L channel\n",
        "                plt.subplot(len(class_names) * num_samples, 4, row_base * 4 + 2)\n",
        "                plt.imshow(l_channel, cmap='gray')\n",
        "                plt.title(\"L Channel (Lightness)\")\n",
        "                plt.axis('off')\n",
        "                \n",
        "                # A channel\n",
        "                plt.subplot(len(class_names) * num_samples, 4, row_base * 4 + 3)\n",
        "                plt.imshow(a_channel, cmap='RdYlGn')\n",
        "                plt.title(\"A Channel (Green-Red)\")\n",
        "                plt.axis('off')\n",
        "                \n",
        "                # B channel\n",
        "                plt.subplot(len(class_names) * num_samples, 4, row_base * 4 + 4)\n",
        "                plt.imshow(b_channel, cmap='coolwarm')\n",
        "                plt.title(\"B Channel (Blue-Yellow)\")\n",
        "                plt.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(\"LAB Color Space Visualization by Wood Type\", y=1.02, fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "# Visualize LAB channels\n",
        "visualize_lab_channels(all_images, all_labels, CLASS_NAMES)\n",
        "\n",
        "# Calculate and visualize average LAB values per class\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Calculate average channel values per class\n",
        "avg_values = []\n",
        "for i, class_name in enumerate(CLASS_NAMES):\n",
        "    class_indices = np.where(all_labels == i)[0]\n",
        "    class_images = all_images[class_indices]\n",
        "    \n",
        "    # Calculate average per channel\n",
        "    avg_l = np.mean(class_images[:,:,:,0])\n",
        "    avg_a = np.mean(class_images[:,:,:,1])\n",
        "    avg_b = np.mean(class_images[:,:,:,2])\n",
        "    \n",
        "    avg_values.append([avg_l, avg_a, avg_b])\n",
        "\n",
        "# Convert to numpy array for easier plotting\n",
        "avg_values = np.array(avg_values)\n",
        "\n",
        "# Plot average values for each channel\n",
        "channels = ['L (Lightness)', 'A (Green-Red)', 'B (Blue-Yellow)']\n",
        "for c in range(3):\n",
        "    plt.subplot(1, 3, c+1)\n",
        "    plt.bar(CLASS_NAMES, avg_values[:,c])\n",
        "    plt.title(f\"Average {channels[c]} Value\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa11bb22-cc33-dd44-ee55-66778899ff00",
      "metadata": {},
      "source": ["### 2.4 Split Data into Training, Validation, and Test Sets"]
    },
    {
      "cell_type": "code",
      "id": "bb22cc33-dd44-ee55-ff66-778899aabbcc",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into train, validation, and test sets (70%/15%/15% split)\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "    all_images, all_labels, test_size=0.3, random_state=RANDOM_SEED, stratify=all_labels\n",
        ")\n",
        "\n",
        "val_images, test_images, val_labels, test_labels = train_test_split(\n",
        "    test_images, test_labels, test_size=0.5, random_state=RANDOM_SEED, stratify=test_labels\n",
        ")\n",
        "\n",
        "print(f\"Training set: {train_images.shape[0]} images\")\n",
        "print(f\"Validation set: {val_images.shape[0]} images\")\n",
        "print(f\"Testing set: {test_images.shape[0]} images\")\n",
        "\n",
        "# Convert labels to categorical (one-hot encoding)\n",
        "train_labels_cat = to_categorical(train_labels, NUM_CLASSES)\n",
        "val_labels_cat = to_categorical(val_labels, NUM_CLASSES)\n",
        "test_labels_cat = to_categorical(test_labels, NUM_CLASSES)\n",
        "\n",
        "# Function to display class distribution\n",
        "def show_class_distribution(labels, title=\"Class Distribution\"):\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    distribution = dict(zip([CLASS_NAMES[int(i)] for i in unique], counts))\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(distribution.keys(), distribution.values(), color='skyblue')\n",
        "    plt.title(title)\n",
        "    plt.ylabel('Number of Images')\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    # Add counts on top of bars\n",
        "    for i, count in enumerate(distribution.values()):\n",
        "        plt.text(i, count + 5, str(count), ha='center')\n",
        "        \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print percentages\n",
        "    print(f\"\\n{title}:\")\n",
        "    total = sum(distribution.values())\n",
        "    for class_name, count in distribution.items():\n",
        "        print(f\"  {class_name}: {count} images ({count/total*100:.1f}%)\")\n",
        "\n",
        "# Display class distributions\n",
        "show_class_distribution(train_labels, \"Training Set Class Distribution\")\n",
        "show_class_distribution(val_labels, \"Validation Set Class Distribution\")\n",
        "show_class_distribution(test_labels, \"Test Set Class Distribution\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc33dd44-ee55-ff66-0011-223344556677",
      "metadata": {},
      "source": ["### 2.5 Data Augmentation"]
    },
    {
      "cell_type": "code",
      "id": "dd44ee55-ff66-0011-2233-445566778899",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up data augmentation for training\n",
        "data_augmentation = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.9, 1.1],  # Be careful with brightness in LAB space\n",
        "    zoom_range=0.1,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Visualize augmented LAB images\n",
        "def show_augmented_lab_images(original_image):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        if i == 0:\n",
        "            sample_image = original_image[0:1]  # Select first image and keep dimensions\n",
        "            plt.title('Original')\n",
        "        else:\n",
        "            # Create augmented image\n",
        "            batch = data_augmentation.flow(original_image[0:1], batch_size=1)\n",
        "            augmented_images = batch.next()\n",
        "            sample_image = augmented_images[0]\n",
        "            plt.title(f'Augmented #{i}')\n",
        "\n",
        "        # Convert LAB back to RGB for display\n",
        "        img = sample_image.copy()\n",
        "        # Denormalize\n",
        "        l = img[:,:,0] * 100\n",
        "        a = img[:,:,1] * 255 - 127\n",
        "        b = img[:,:,2] * 255 - 127\n",
        "        # Reconstruct LAB image (OpenCV format)\n",
        "        lab_img_cv = np.stack([l, a, b], axis=-1).astype(np.uint8)\n",
        "        # Convert back to RGB for display\n",
        "        rgb_img = cv2.cvtColor(lab_img_cv, cv2.COLOR_LAB2RGB)\n",
        "        plt.imshow(rgb_img)\n",
        "        plt.axis('off')\n",
        "        \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Select a sample image (first image from training set)\n",
        "sample_img = train_images[0:1]\n",
        "\n",
        "# Visualize augmentation\n",
        "print(\"LAB Image Augmentation Examples:\")\n",
        "show_augmented_lab_images(sample_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee55ff66-0011-2233-4455-66778899aabb",
      "metadata": {},
      "source": [
        "## 3. Model Development and Training\n",
        "\n",
        "### 3.1 Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "id": "ff667700-1122-3344-5566-77889900aabb",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=NUM_CLASSES):\n",
        "    \"\"\"\n",
        "    Build a wood classification model using transfer learning with MobileNetV2\n",
        "    \"\"\"\n",
        "    # Base model - MobileNetV2\n",
        "    base_model = MobileNetV2(\n",
        "        input_shape=input_shape,\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "    \n",
        "    # Freeze the base model layers\n",
        "    base_model.trainable = False\n",
        "    \n",
        "    # Create the model\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Build the model\n",
        "model = build_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00112233-4455-6677-8899-aabbccddeeff",
      "metadata": {},
      "source": ["### 3.2 Training the Model"]
    },
    {
      "cell_type": "code",
      "id": "11223344-5566-7788-99aa-bbccddeeff00",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define callbacks\n",
        "def get_callbacks():\n",
        "    return [\n",
        "        callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-6\n",
        "        ),\n",
        "        callbacks.ModelCheckpoint(\n",
        "            'wood_classifier_lab_best.h5',\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            mode='max'\n",
        "        )\n",
        "    ]\n",
        "\n",
        "# Train the model\n",
        "print(\"Training the model...\")\n",
        "history = model.fit(\n",
        "    data_augmentation.flow(train_images, train_labels_cat, batch_size=BATCH_SIZE),\n",
        "    validation_data=(val_images, val_labels_cat),\n",
        "    epochs=30,\n",
        "    callbacks=get_callbacks(),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "def plot_training_history(history):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    \n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22334455-6677-8899-aabb-ccddeeff0011",
      "metadata": {},
      "source": ["### 3.3 Fine-tuning the Model"]
    },
    {
      "cell_type": "code",
      "id": "33445566-7788-99aa-bbcc-ddeeff001122",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tune the model\n",
        "print(\"Fine-tuning the model...\")\n",
        "\n",
        "# Unfreeze the top layers of the base model\n",
        "base_model = model.layers[0]\n",
        "base_model.trainable = True\n",
        "\n",
        "# Freeze the bottom layers and unfreeze the top layers\n",
        "for layer in base_model.layers[:-30]:  # Keep the bottom layers frozen\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[-30:]:  # Unfreeze the top layers\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile with a lower learning rate\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-5),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Continue training\n",
        "fine_tune_history = model.fit(\n",
        "    data_augmentation.flow(train_images, train_labels_cat, batch_size=BATCH_SIZE // 2),\n",
        "    validation_data=(val_images, val_labels_cat),\n",
        "    epochs=20,\n",
        "    callbacks=get_callbacks(),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot fine-tuning history\n",
        "plot_training_history(fine_tune_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44556677-8899-aabb-ccdd-eeff00112233",
      "metadata": {},
      "source": [
        "## 4. Model Evaluation\n",
        "\n",
        "Let's evaluate our model on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "id": "55667788-99aa-bbcc-ddee-ff0011223344",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels_cat)\n",
        "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test loss: {test_loss:.4f}\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_prob = model.predict(test_images)\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "report = classification_report(test_labels, y_pred, target_names=CLASS_NAMES)\n",
        "print(report)\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(test_labels, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=CLASS_NAMES,\n",
        "            yticklabels=CLASS_NAMES)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize predictions\n",
        "def visualize_predictions(images, true_labels, pred_labels, class_names, num_samples=10):\n",
        "    # Find both correct and incorrect predictions\n",
        "    correct_indices = np.where(true_labels == pred_labels)[0]\n",
        "    incorrect_indices = np.where(true_labels != pred_labels)[0]\n",
        "    \n",
        "    # Determine how many of each to show\n",
        "    n_incorrect = min(num_samples // 2, len(incorrect_indices))\n",
        "    n_correct = min(num_samples - n_incorrect, len(correct_indices))\n",
        "    \n",
        "    # Select random samples\n",
        "    if len(incorrect_indices) > 0:\n",
        "        selected_incorrect = np.random.choice(incorrect_indices, n_incorrect, replace=False)\n",
        "    else:\n",
        "        selected_incorrect = []\n",
        "    \n",
        "    selected_correct = np.random.choice(correct_indices, n_correct, replace=False)\n",
        "    \n",
        "    # Combine indices\n",
        "    selected_indices = np.concatenate([selected_incorrect, selected_correct])\n",
        "    \n",
        "    # Create figure\n",
        "    plt.figure(figsize=(15, 2 * ((len(selected_indices) + 4) // 5)))\n",
        "    \n",
        "    for i, idx in enumerate(selected_indices):\n",
        "        img = images[idx]\n",
        "        \n",
        "        # Convert LAB back to RGB for display\n",
        "        l = img[:,:,0] * 100\n",
        "        a = img[:,:,1] * 255 - 127\n",
        "        b = img[:,:,2] * 255 - 127\n",
        "        lab_img_cv = np.stack([l, a, b], axis=-1).astype(np.uint8)\n",
        "        rgb_img = cv2.cvtColor(lab_img_cv, cv2.COLOR_LAB2RGB)\n",
        "        \n",
        "        plt.subplot(((len(selected_indices) + 4) // 5), 5, i + 1)\n",
        "        plt.imshow(rgb_img)\n",
        "        \n",
        "        true_class = class_names[true_labels[idx]]\n",
        "        pred_class = class_names[pred_labels[idx]]\n",
        "        \n",
        "        if true_labels[idx] == pred_labels[idx]:\n",
        "            color = 'green'\n",
        "        else:\n",
        "            color = 'red'\n",
        "        \n",
        "        plt.title(f\"True: {true_class}\\nPred: {pred_class}\", color=color, fontsize=10)\n",
        "        plt.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize sample predictions\n",
        "visualize_predictions(test_images, test_labels, y_pred, CLASS_NAMES)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66778899-aabb-ccdd-eeff-001122334455",
      "metadata": {},
      "source": ["## 5. Save and Convert Model for Deployment"]
    },
    {
      "cell_type": "code",
      "id": "77889900-aabb-ccdd-eeff-112233445566",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the Keras model\n",
        "model.save('wood_classifier_lab.h5')\n",
        "print(\"Saved Keras model to wood_classifier_lab.h5\")\n",
        "\n",
        "# Convert to TensorFlow Lite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TFLite model\n",
        "with open('wood_classifier_lab.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "print(\"Saved TFLite model to wood_classifier_lab.tflite\")\n",
        "\n",
        "# Convert to optimized TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_optimized_model = converter.convert()\n",
        "\n",
        "# Save the optimized TFLite model\n",
        "with open('wood_classifier_lab_optimized.tflite', 'wb') as f:\n",
        "    f.write(tflite_optimized_model)\n",
        "print(\"Saved optimized TFLite model to wood_classifier_lab_optimized.tflite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8899aabb-ccdd-eeff-0011-223344556677",
      "metadata": {},
      "source": ["## 6. Test the TFLite Model"]
    },
    {
      "cell_type": "code",
      "id": "99aabbcc-ddee-0011-2233-445566778899",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the TFLite model and allocate tensors\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print(\"Input details:\", input_details)\n",
        "print(\"Output details:\", output_details)\n",
        "\n",
        "# Test the TFLite model on a few images\n",
        "def test_tflite_model(interpreter, images, labels, class_names, num_samples=5):\n",
        "    # Get input and output details\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    \n",
        "    # Create a figure for visualization\n",
        "    plt.figure(figsize=(15, 3 * num_samples))\n",
        "    \n",
        "    # Test on random samples\n",
        "    indices = np.random.choice(range(len(images)), num_samples, replace=False)\n",
        "    \n",
        "    for i, idx in enumerate(indices):\n",
        "        # Process input\n",
        "        input_image = images[idx:idx+1]\n",
        "        interpreter.set_tensor(input_details[0]['index'], input_image)\n",
        "        \n",
        "        # Run inference\n",
        "        interpreter.invoke()\n",
        "        \n",
        "        # Get the output\n",
        "        output = interpreter.get_tensor(output_details[0]['index'])\n",
        "        predicted_class = np.argmax(output[0])\n",
        "        \n",
        "        # Calculate confidence\n",
        "        confidence = output[0][predicted_class] * 100\n",
        "        \n",
        "        # Display results\n",
        "        plt.subplot(num_samples, 1, i+1)\n",
        "        \n",
        "        # Convert LAB back to RGB for display\n",
        "        img = images[idx].copy()\n",
        "        l = img[:,:,0] * 100\n",
        "        a = img[:,:,1] * 255 - 127\n",
        "        b = img[:,:,2] * 255 - 127\n",
        "        lab_img_cv = np.stack([l, a, b], axis=-1).astype(np.uint8)\n",
        "        rgb_img = cv2.cvtColor(lab_img_cv, cv2.COLOR_LAB2RGB)\n",
        "        plt.imshow(rgb_img)\n",
        "        \n",
        "        title = f\"True: {class_names[labels[idx]]}\\n\"\n",
        "        title += f\"Predicted: {class_names[predicted_class]} ({confidence:.1f}%)\"\n",
        "        \n",
        "        if predicted_class == labels[idx]:\n",
        "            plt.title(title, color='green')\n",
        "        else:\n",
        "            plt.title(title, color='red')\n",
        "        \n",
        "        plt.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Test the TFLite model\n",
        "test_tflite_model(interpreter, test_images, test_labels, CLASS_NAMES)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aabbccdd-eeff-0011-2233-4455667788aa",
      "metadata": {},
      "source": [
        "## 7. Using the Model in Production\n",
        "\n",
        "To use this model in a production environment, you'll need to:\n",
        "\n",
        "1. Preprocess new images by converting to LAB color space\n",
        "2. Normalize the LAB values appropriately\n",
        "3. Run the model on the preprocessed image\n",
        "\n",
        "Here's a function you can use to preprocess new images:"
      ]
    },
    {
      "cell_type": "code",
      "id": "bbccddee-ff00-1122-3344-5566778899bb",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_image_for_model(image_path, img_size=224):\n",
        "    \"\"\"\n",
        "    Preprocess an image for the wood classifier model\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to the image file\n",
        "        img_size: Size to resize the image to\n",
        "        \n",
        "    Returns:\n",
        "        preprocessed_image: LAB image normalized and ready for the model\n",
        "    \"\"\"\n",
        "    # Read image\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Could not read image: {image_path}\")\n",
        "    \n",
        "    # Convert BGR to RGB\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Resize to target dimensions\n",
        "    img_resized = cv2.resize(img_rgb, (img_size, img_size))\n",
        "    \n",
        "    # Convert RGB to LAB\n",
        "    img_lab = cv2.cvtColor(img_resized, cv2.COLOR_RGB2LAB)\n",
        "    \n",
        "    # Normalize LAB values\n",
        "    l_channel = img_lab[:,:,0] / 100.0\n",
        "    a_channel = (img_lab[:,:,1] + 127) / 255.0\n",
        "    b_channel = (img_lab[:,:,2] + 127) / 255.0\n",
        "    \n",
        "    # Stack normalized channels\n",
        "    normalized_lab = np.stack([l_channel, a_channel, b_channel], axis=-1)\n",
        "    \n",
        "    # Add batch dimension\n",
        "    return np.expand_dims(normalized_lab, axis=0)\n",
        "\n",
        "# Example of using the model with a new image\n",
        "def classify_new_image(image_path, model, class_names):\n",
        "    \"\"\"\n",
        "    Classify a new image using the trained model\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to the image file\n",
        "        model: Trained Keras model\n",
        "        class_names: List of class names\n",
        "        \n",
        "    Returns:\n",
        "        predicted_class: The predicted class name\n",
        "        confidence: Confidence score (0-100%)\n",
        "    \"\"\"\n",
        "    # Preprocess the image\n",
        "    preprocessed_img = preprocess_image_for_model(image_path)\n",
        "    \n",
        "    # Make prediction\n",
        "    predictions = model.predict(preprocessed_img)\n",
        "    \n",
        "    # Get the predicted class and confidence\n",
        "    predicted_class_idx = np.argmax(predictions[0])\n",
        "    predicted_class = class_names[predicted_class_idx]\n",
        "    confidence = predictions[0][predicted_class_idx] * 100\n",
        "    \n",
        "    return predicted_class, confidence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccddee11-2233-4455-6677-8899aabbccdd",
      "metadata": {},
      "source": [
        "## 8. Conclusion\n",
        "\n",
        "This notebook has demonstrated how to build a wood finish classification model using LAB color space, which is particularly well-suited for distinguishing between different wood tones. The LAB color space separates lightness (L channel) from color information (a and b channels), making it easier for the model to distinguish subtle color differences in wood finishes.\n",
        "\n",
        "Key takeaways:\n",
        "1. LAB color space provides better feature separation for wood finish classification\n",
        "2. Data augmentation helps improve model robustness\n",
        "3. Transfer learning with MobileNetV2 provides a strong foundation\n",
        "4. Fine-tuning further improves model performance\n",
        "\n",
        "The trained model can now be deployed on mobile or edge devices using the optimized TensorFlow Lite format."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
