{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Material Color Range Validation - Model Comparison\n",
    "\n",
    "This notebook demonstrates how to build and compare five different AI models to determine whether an image of a material is **in-range** or **out-of-range** based on its color/lightness. We will use transfer learning with a pre-trained convolutional neural network (CNN) backbone and evaluate the following approaches:\n",
    "\n",
    "- **Multi-Class Classification** – 5-way classification (in-range: light, standard, dark; out-of-range: too light, too dark).\n",
    "- **Binary Classification** – 2-way classification (in-range vs. out-of-range).\n",
    "- **Regression** – Predict a continuous brightness score and threshold it to classify range.\n",
    "- **Ordinal Regression** – Treat the problem as an ordinal classification, exploiting the ordered nature of brightness levels.\n",
    "- **Hybrid Multi-Task** – A model with two heads: one for 5-class classification and one for regression, sharing a common CNN base.\n",
    "\n",
    "We will load images from directories corresponding to the five categories, apply preprocessing and augmentation, then train and fine-tune each model. Finally, we evaluate the models on a test set, compare their accuracies, and export the models to TensorFlow Lite for deployment.\n",
    "\n",
    "> **Note:** We set random seeds for reproducibility and optimize the workflow for performance (e.g., using caching and prefetching for efficient data loading). The code is written modularly so that you can easily adapt it to different materials by changing the dataset path or material name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcb8e06-4f10-4b09-b847-79a25bcb29f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define the material dataset directory (change material_name to reuse for a different material)\n",
    "material_name = \"MediumCherry\"\n",
    "base_data_dir = pathlib.Path(\"/path/to/dataset\") / material_name  # <-- update this path as needed\n",
    "\n",
    "# Define class names in order of increasing darkness (brightness decreases from first to last)\n",
    "class_names = [\"out_of_range_too_light\", \"in_range_light\", \"in_range_standard\", \"in_range_dark\", \"out_of_range_too_dark\"]\n",
    "\n",
    "# Verify that the directories exist\n",
    "for cname in class_names:\n",
    "    if not (base_data_dir / cname).exists():\n",
    "        raise FileNotFoundError(f\"Directory not found: {base_data_dir/cname} (please check the path and folder names)\")\n",
    "\n",
    "# Collect all image file paths and their class labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "for idx, cname in enumerate(class_names):\n",
    "    for filepath in (base_data_dir / cname).glob(\"*.*\"):\n",
    "        image_paths.append(str(filepath))\n",
    "        labels.append(idx)\n",
    "image_paths = np.array(image_paths)\n",
    "labels = np.array(labels)\n",
    "num_images = len(labels)\n",
    "print(f\"Found {num_images} images for material '{material_name}'.\")\n",
    "\n",
    "# Shuffle and split into train/validation/test (80/10/10 split)\n",
    "indices = np.arange(num_images)\n",
    "np.random.shuffle(indices)\n",
    "train_end = int(0.8 * num_images)\n",
    "val_end = int(0.9 * num_images)\n",
    "train_indices = indices[:train_end]\n",
    "val_indices   = indices[train_end:val_end]\n",
    "test_indices  = indices[val_end:]\n",
    "train_paths, train_labels = image_paths[train_indices], labels[train_indices]\n",
    "val_paths,   val_labels   = image_paths[val_indices],   labels[val_indices]\n",
    "test_paths,  test_labels  = image_paths[test_indices],  labels[test_indices]\n",
    "print(f\"Split: {len(train_labels)} training, {len(val_labels)} validation, {len(test_labels)} test images.\")\n",
    "\n",
    "# Create TensorFlow Dataset objects from file paths and labels\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "val_ds   = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "test_ds  = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
    "\n",
    "# Function to load and resize an image from a file path\n",
    "def load_and_resize(image_path, label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_image(image, channels=3, expand_animations=False)  # decode JPEG/PNG/etc.\n",
    "    image = tf.image.resize(image, [224, 224])  # resize to 224x224\n",
    "    image = tf.cast(image, tf.float32)         # convert to float32\n",
    "    return image, label\n",
    "\n",
    "# Apply the loading function and cache the data (to RAM) for faster reuse\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_ds = train_ds.map(load_and_resize, num_parallel_calls=AUTOTUNE).cache()\n",
    "val_ds   = val_ds.map(load_and_resize, num_parallel_calls=AUTOTUNE).cache()\n",
    "test_ds  = test_ds.map(load_and_resize, num_parallel_calls=AUTOTUNE).cache()\n",
    "\n",
    "# Define data augmentation pipeline for training images\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),           # random horizontal flip\n",
    "    layers.RandomRotation(0.05),                   # random rotation (±5%)\n",
    "    layers.RandomZoom(0.1),                        # random zoom\n",
    "    # layers.RandomTranslation(...) could be added for shifts\n",
    "])\n",
    "# Note: We avoid color/brightness augmentations, as those would alter the label (brightness range).\n",
    "\n",
    "# Normalize pixel values from [0,255] to [0,1]\n",
    "normalization_layer = layers.Rescaling(1./255)\n",
    "\n",
    "# Apply augmentation (training only) and normalization\n",
    "train_ds = train_ds.map(lambda img, lbl: (data_augmentation(img, training=True), lbl), num_parallel_calls=AUTOTUNE)\n",
    "train_ds = train_ds.map(lambda img, lbl: (normalization_layer(img), lbl), num_parallel_calls=AUTOTUNE)\n",
    "val_ds   = val_ds.map(lambda img, lbl: (normalization_layer(img), lbl), num_parallel_calls=AUTOTUNE)\n",
    "test_ds  = test_ds.map(lambda img, lbl: (normalization_layer(img), lbl), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Batch and prefetch the datasets for efficiency\n",
    "batch_size = 32\n",
    "train_ds = train_ds.shuffle(buffer_size=1000, seed=42, reshuffle_each_iteration=True)\n",
    "train_ds = train_ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "val_ds   = val_ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "test_ds  = test_ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "\n",
    "# Define brightness values for each class (for regression target)\n",
    "# These are hypothetical \"brightness scores\" for each category, 0=darkest, 1=brightest\n",
    "brightness_values = tf.constant([0.9, 0.75, 0.5, 0.25, 0.1], dtype=tf.float32)\n",
    "# Define thresholds for in-range vs out-of-range based on the midpoints between categories\n",
    "upper_threshold = (float(brightness_values[0]) + float(brightness_values[1])) / 2.0  # between \"too_light\" and \"in_range_light\"\n",
    "lower_threshold = (float(brightness_values[-2]) + float(brightness_values[-1])) / 2.0  # between \"in_range_dark\" and \"too_dark\"\n",
    "print(f\"Brightness thresholds for 'in-range': lower={lower_threshold:.3f}, upper={upper_threshold:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Development\n",
    "\n",
    "We use transfer learning with EfficientNetB0 as the CNN base. For each model, we freeze the base initially and add new layers on top appropriate for the task.\n",
    "\n",
    "### a. Multi-Class Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b087b0e2-8a16-49f7-8b3f-6a0a42b6a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Multi-Class Classification Model\n",
    "# Base CNN model (pre-trained EfficientNetB0)\n",
    "base_model_multi = tf.keras.applications.EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
    "base_model_multi.trainable = False  # freeze base model layers initially\n",
    "\n",
    "# Add top layers for multi-class classification\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "x = base_model_multi(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.2)(x)  # dropout for regularization\n",
    "output_multi = layers.Dense(5, activation=\"softmax\")(x)  # 5 classes softmax\n",
    "\n",
    "model_multi = keras.Model(inputs, output_multi, name=\"MultiClassModel\")\n",
    "model_multi.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                    loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_multi.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Binary Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b78a2-299a-4677-8ed4-1a09e83b59d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Binary Classification Model\n",
    "base_model_bin = tf.keras.applications.EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
    "base_model_bin.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "x = base_model_bin(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "output_bin = layers.Dense(1, activation=\"sigmoid\")(x)  # single sigmoid output\n",
    "\n",
    "model_binary = keras.Model(inputs, output_bin, name=\"BinaryModel\")\n",
    "model_binary.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                     loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_binary.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc27b4-4c56-4c52-9bb0-b43ad8ef4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Regression Model\n",
    "base_model_reg = tf.keras.applications.EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
    "base_model_reg.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "x = base_model_reg(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "output_reg = layers.Dense(1, activation=\"linear\")(x)  # linear output for brightness score\n",
    "\n",
    "model_regression = keras.Model(inputs, output_reg, name=\"RegressionModel\")\n",
    "model_regression.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                         loss=\"mse\")\n",
    "model_regression.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Ordinal Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f9a0d4-8d0a-4e2b-88cf-dcbfd20663c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Ordinal Regression Model\n",
    "base_model_ord = tf.keras.applications.EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
    "base_model_ord.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "x = base_model_ord(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "output_ord = layers.Dense(4, activation=\"sigmoid\")(x)  # 4 sigmoid outputs for ordinal thresholds\n",
    "\n",
    "model_ordinal = keras.Model(inputs, output_ord, name=\"OrdinalModel\")\n",
    "model_ordinal.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                      loss=\"binary_crossentropy\")\n",
    "model_ordinal.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Hybrid Multi-Task Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b09e8a-fbda-4766-97c3-0f0f37387982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e. Hybrid Multi-Task Model\n",
    "base_model_mt = tf.keras.applications.EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
    "base_model_mt.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "x = base_model_mt(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "## Classification head (5 classes)\n",
    "class_output = layers.Dense(5, activation=\"softmax\", name=\"class_output\")(x)\n",
    "## Regression head (brightness)\n",
    "reg_output = layers.Dense(1, activation=\"linear\", name=\"reg_output\")(x)\n",
    "\n",
    "model_multi_task = keras.Model(inputs, [class_output, reg_output], name=\"MultiTaskModel\")\n",
    "model_multi_task.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                         loss={\"class_output\": \"categorical_crossentropy\", \"reg_output\": \"mse\"},\n",
    "                         metrics={\"class_output\": \"accuracy\"})\n",
    "model_multi_task.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training and Fine-Tuning\n",
    "\n",
    "For each model, we first train only the new top layers (with the EfficientNet base frozen). Then we unfreeze the last few layers of the base for fine-tuning using a lower learning rate.\n",
    "\n",
    "### Multi-Class Model Training & Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5ab43-75f4-40d3-bd03-d216ba3b8716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Multi-Class Classification model\n",
    "epochs_initial = 5\n",
    "epochs_finetune = 5\n",
    "\n",
    "# One-hot encode the training and validation labels for 5-class classification\n",
    "train_ds_multi = train_ds.map(lambda x, y: (x, tf.one_hot(y, depth=5)))\n",
    "val_ds_multi = val_ds.map(lambda x, y: (x, tf.one_hot(y, depth=5)))\n",
    "\n",
    "# Initial training (base frozen)\n",
    "history_multi = model_multi.fit(train_ds_multi, validation_data=val_ds_multi, epochs=epochs_initial)\n",
    "\n",
    "# Fine-tuning: unfreeze last few layers of the base model\n",
    "base_model_multi.trainable = True\n",
    "# Freeze all layers except the last 20 layers in the base model (as an example)\n",
    "for layer in base_model_multi.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "# Recompile with a lower learning rate for fine-tuning\n",
    "model_multi.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "                    loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# Continue training\n",
    "history_multi_ft = model_multi.fit(train_ds_multi, validation_data=val_ds_multi,\n",
    "                                   epochs=epochs_initial+epochs_finetune, initial_epoch=history_multi.epoch[-1] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f0510-1d33-4de0-bb2b-d99c74a3dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Combine history from initial and fine-tune phases\n",
    "acc = history_multi.history['accuracy'] + history_multi_ft.history.get('accuracy', [])\n",
    "val_acc = history_multi.history['val_accuracy'] + history_multi_ft.history.get('val_accuracy', [])\n",
    "loss = history_multi.history['loss'] + history_multi_ft.history.get('loss', [])\n",
    "val_loss = history_multi.history['val_loss'] + history_multi_ft.history.get('val_loss', [])\n",
    "\n",
    "epochs_range = range(1, len(acc) + 1)\n",
    "plt.figure(figsize=(8,4))\n",
    "# Plot loss\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs_range, loss, label='Train Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Val Loss')\n",
    "plt.title('Multi-Class Model Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "# Plot accuracy\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs_range, acc, label='Train Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Val Accuracy')\n",
    "plt.title('Multi-Class Model Accuracy')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Model Training & Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3178a4-7eb5-4d22-8c55-b0de264a50aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Binary Classification model\n",
    "epochs_initial = 5\n",
    "epochs_finetune = 5\n",
    "\n",
    "# Prepare binary labels: 1 for in-range (classes 1,2,3), 0 for out-of-range (classes 0,4)\n",
    "train_ds_bin = train_ds.map(lambda x, y: (x, tf.cast((y >= 1) & (y <= 3), tf.float32)))\n",
    "val_ds_bin = val_ds.map(lambda x, y: (x, tf.cast((y >= 1) & (y <= 3), tf.float32)))\n",
    "\n",
    "history_bin = model_binary.fit(train_ds_bin, validation_data=val_ds_bin, epochs=epochs_initial)\n",
    "\n",
    "# Fine-tune base model (unfreeze some layers)\n",
    "base_model_bin.trainable = True\n",
    "for layer in base_model_bin.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "model_binary.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "                     loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "history_bin_ft = model_binary.fit(train_ds_bin, validation_data=val_ds_bin,\n",
    "                                  epochs=epochs_initial+epochs_finetune, initial_epoch=history_bin.epoch[-1] + 1)\n",
    "\n",
    "# Plot training curves for binary model\n",
    "acc_bin = history_bin.history['accuracy'] + history_bin_ft.history.get('accuracy', [])\n",
    "val_acc_bin = history_bin.history['val_accuracy'] + history_bin_ft.history.get('val_accuracy', [])\n",
    "loss_bin = history_bin.history['loss'] + history_bin_ft.history.get('loss', [])\n",
    "val_loss_bin = history_bin.history['val_loss'] + history_bin_ft.history.get('val_loss', [])\n",
    "epochs_range_bin = range(1, len(acc_bin) + 1)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs_range_bin, loss_bin, label='Train Loss')\n",
    "plt.plot(epochs_range_bin, val_loss_bin, label='Val Loss')\n",
    "plt.title('Binary Model Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs_range_bin, acc_bin, label='Train Accuracy')\n",
    "plt.plot(epochs_range_bin, val_acc_bin, label='Val Accuracy')\n",
    "plt.title('Binary Model Accuracy')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Model Training & Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c7894-3e80-4148-b68a-d0aa4dd60ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Regression model\n",
    "epochs_initial = 5\n",
    "epochs_finetune = 5\n",
    "\n",
    "# Prepare regression labels (brightness scores)\n",
    "train_ds_reg = train_ds.map(lambda x, y: (x, tf.gather(brightness_values, tf.cast(y, tf.int32))))\n",
    "val_ds_reg = val_ds.map(lambda x, y: (x, tf.gather(brightness_values, tf.cast(y, tf.int32))))\n",
    "\n",
    "history_reg = model_regression.fit(train_ds_reg, validation_data=val_ds_reg, epochs=epochs_initial)\n",
    "\n",
    "# Fine-tune base model\n",
    "base_model_reg.trainable = True\n",
    "for layer in base_model_reg.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "model_regression.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5), loss=\"mse\")\n",
    "history_reg_ft = model_regression.fit(train_ds_reg, validation_data=val_ds_reg,\n",
    "                                      epochs=epochs_initial+epochs_finetune, initial_epoch=history_reg.epoch[-1] + 1)\n",
    "\n",
    "# Plot training curves for regression model\n",
    "loss_reg = history_reg.history['loss'] + history_reg_ft.history.get('loss', [])\n",
    "val_loss_reg = history_reg.history['val_loss'] + history_reg_ft.history.get('val_loss', [])\n",
    "epochs_range_reg = range(1, len(loss_reg) + 1)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(epochs_range_reg, loss_reg, label='Train MSE')\n",
    "plt.plot(epochs_range_reg, val_loss_reg, label='Val MSE')\n",
    "plt.title('Regression Model Training (MSE)')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Mean Squared Error'); plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Model Training & Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c76b4d9-4e63-4f90-b56e-1ec180b454a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Ordinal regression model\n",
    "epochs_initial = 5\n",
    "epochs_finetune = 5\n",
    "\n",
    "# Prepare ordinal labels (4 binary values) from class label\n",
    "def ordinal_targets(label):\n",
    "    # label is scalar tensor (0-4), compare with [0,1,2,3]\n",
    "    return tf.cast(label > tf.range(4, dtype=tf.int32), tf.float32)\n",
    "\n",
    "train_ds_ord = train_ds.map(lambda x, y: (x, ordinal_targets(y)))\n",
    "val_ds_ord = val_ds.map(lambda x, y: (x, ordinal_targets(y)))\n",
    "\n",
    "history_ord = model_ordinal.fit(train_ds_ord, validation_data=val_ds_ord, epochs=epochs_initial)\n",
    "\n",
    "# Fine-tune base model\n",
    "base_model_ord.trainable = True\n",
    "for layer in base_model_ord.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "model_ordinal.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5), loss=\"binary_crossentropy\")\n",
    "history_ord_ft = model_ordinal.fit(train_ds_ord, validation_data=val_ds_ord,\n",
    "                                   epochs=epochs_initial+epochs_finetune, initial_epoch=history_ord.epoch[-1] + 1)\n",
    "\n",
    "# Plot training curves for ordinal model\n",
    "loss_ord = history_ord.history['loss'] + history_ord_ft.history.get('loss', [])\n",
    "val_loss_ord = history_ord.history['val_loss'] + history_ord_ft.history.get('val_loss', [])\n",
    "epochs_range_ord = range(1, len(loss_ord) + 1)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(epochs_range_ord, loss_ord, label='Train Loss')\n",
    "plt.plot(epochs_range_ord, val_loss_ord, label='Val Loss')\n",
    "plt.title('Ordinal Model Training Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Binary Crossentropy Loss'); plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Task Model Training & Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb48f54-c05f-46c7-bfc8-1bf11970093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Hybrid Multi-Task model\n",
    "epochs_initial = 5\n",
    "epochs_finetune = 5\n",
    "\n",
    "# Prepare multi-task labels: (one-hot class, brightness)\n",
    "train_ds_mt = train_ds.map(lambda x, y: (x, (tf.one_hot(y, depth=5), tf.gather(brightness_values, tf.cast(y, tf.int32)))))\n",
    "val_ds_mt = val_ds.map(lambda x, y: (x, (tf.one_hot(y, depth=5), tf.gather(brightness_values, tf.cast(y, tf.int32)))))\n",
    "\n",
    "history_mt = model_multi_task.fit(train_ds_mt, validation_data=val_ds_mt, epochs=epochs_initial)\n",
    "\n",
    "# Fine-tune base model\n",
    "base_model_mt.trainable = True\n",
    "for layer in base_model_mt.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "model_multi_task.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "                         loss={\"class_output\": \"categorical_crossentropy\", \"reg_output\": \"mse\"},\n",
    "                         metrics={\"class_output\": \"accuracy\"})\n",
    "history_mt_ft = model_multi_task.fit(train_ds_mt, validation_data=val_ds_mt,\n",
    "                                     epochs=epochs_initial+epochs_finetune, initial_epoch=history_mt.epoch[-1] + 1)\n",
    "\n",
    "# Plot training curves for multi-task model (classification head performance)\n",
    "acc_mt = history_mt.history['class_output_accuracy'] + history_mt_ft.history.get('class_output_accuracy', [])\n",
    "val_acc_mt = history_mt.history['val_class_output_accuracy'] + history_mt_ft.history.get('val_class_output_accuracy', [])\n",
    "loss_mt = history_mt.history['loss'] + history_mt_ft.history.get('loss', [])\n",
    "val_loss_mt = history_mt.history['val_loss'] + history_mt_ft.history.get('val_loss', [])\n",
    "epochs_range_mt = range(1, len(acc_mt) + 1)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs_range_mt, loss_mt, label='Train Total Loss')\n",
    "plt.plot(epochs_range_mt, val_loss_mt, label='Val Total Loss')\n",
    "plt.title('Multi-Task Model Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs_range_mt, acc_mt, label='Train Class Accuracy')\n",
    "plt.plot(epochs_range_mt, val_acc_mt, label='Val Class Accuracy')\n",
    "plt.title('Multi-Task Model Classification Accuracy')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation and Comparison\n",
    "\n",
    "We evaluate each model on the test set. For the multi-class and ordinal models, we convert the predictions to a binary decision (in-range vs. out-of-range). In-range is defined as classes 1, 2, or 3, and out-of-range as classes 0 or 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f67f8-5b94-45d8-a0fd-9188cdb0d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "\n",
    "# Multi-class model: get predictions and compute binary accuracy\n",
    "y_pred_multi_proba = model_multi.predict(test_ds_multi)  # probabilities for 5 classes\n",
    "y_pred_multi_class = np.argmax(y_pred_multi_proba, axis=1)\n",
    "y_true_multi_class = test_labels  # original labels 0-4\n",
    "multi_class_accuracy = np.mean(y_pred_multi_class == y_true_multi_class)  # 5-class accuracy\n",
    "\n",
    "# Convert to in-range vs out-of-range (in-range: classes 1,2,3)\n",
    "y_pred_multi_inrange = np.isin(y_pred_multi_class, [1,2,3]).astype(int)\n",
    "y_true_inrange = np.isin(y_true_multi_class, [1,2,3]).astype(int)\n",
    "multi_binary_accuracy = np.mean(y_pred_multi_inrange == y_true_inrange)\n",
    "\n",
    "# Binary model: directly evaluate accuracy on test set\n",
    "loss_bin, binary_accuracy = model_binary.evaluate(test_ds_bin, verbose=0)\n",
    "\n",
    "# Regression model: predict brightness and threshold for classification\n",
    "y_pred_reg = model_regression.predict(test_ds_reg).ravel()  # predicted brightness scores\n",
    "# Classify as in-range (1) if between lower_threshold and upper_threshold, else out-of-range (0)\n",
    "y_pred_reg_inrange = np.where((y_pred_reg >= lower_threshold) & (y_pred_reg <= upper_threshold), 1, 0)\n",
    "y_true_inrange = np.isin(test_labels, [1,2,3]).astype(int)  # reuse from above\n",
    "reg_binary_accuracy = np.mean(y_pred_reg_inrange == y_true_inrange)\n",
    "\n",
    "# Ordinal model: predict and decode ordinal outputs to classes, then to in-range\n",
    "y_pred_ord = model_ordinal.predict(test_ds_ord)\n",
    "y_pred_ord_class = []\n",
    "for pred in y_pred_ord:\n",
    "    ord_class = 4  # default to last class\n",
    "    for k, p in enumerate(pred):\n",
    "        if p < 0.5:\n",
    "            ord_class = k\n",
    "            break\n",
    "    y_pred_ord_class.append(ord_class)\n",
    "y_pred_ord_class = np.array(y_pred_ord_class, dtype=int)\n",
    "ordinal_class_accuracy = np.mean(y_pred_ord_class == y_true_multi_class)  # 5-class accuracy for ordinal\n",
    "# Convert to binary in-range\n",
    "y_pred_ord_inrange = np.isin(y_pred_ord_class, [1,2,3]).astype(int)\n",
    "ord_binary_accuracy = np.mean(y_pred_ord_inrange == y_true_inrange)\n",
    "\n",
    "# Multi-task model: it outputs both class and reg, evaluate classification output\n",
    "y_pred_mt_class_proba = model_multi_task.predict(test_ds_mt)[0]  # model.predict returns [class_probs, reg_pred]\n",
    "y_pred_mt_class = np.argmax(y_pred_mt_class_proba, axis=1)\n",
    "multi_task_class_accuracy = np.mean(y_pred_mt_class == y_true_multi_class)\n",
    "y_pred_mt_inrange = np.isin(y_pred_mt_class, [1,2,3]).astype(int)\n",
    "mt_binary_accuracy = np.mean(y_pred_mt_inrange == y_true_inrange)\n",
    "\n",
    "print(\"Test Accuracy (In-Range vs Out-of-Range):\")\n",
    "print(f\"Multi-Class Model:      {multi_binary_accuracy*100:.2f}%\")\n",
    "print(f\"Binary Classification:  {binary_accuracy*100:.2f}%\")\n",
    "print(f\"Regression Model:       {reg_binary_accuracy*100:.2f}%\")\n",
    "print(f\"Ordinal Regression:     {ord_binary_accuracy*100:.2f}%\")\n",
    "print(f\"Multi-Task Model:       {mt_binary_accuracy*100:.2f}%\")\n",
    "\n",
    "import pandas as pd\n",
    "acc_data = {\n",
    "    \"Model\": [\"Multi-Class\", \"Binary\", \"Regression\", \"Ordinal\", \"Multi-Task\"],\n",
    "    \"Test Accuracy (%)\": [multi_binary_accuracy*100, binary_accuracy*100, reg_binary_accuracy*100, ord_binary_accuracy*100, mt_binary_accuracy*100]\n",
    "}\n",
    "acc_df = pd.DataFrame(acc_data)\n",
    "display(acc_df)\n",
    "\n",
    "# Bar chart of accuracies\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(acc_data[\"Model\"], acc_data[\"Test Accuracy (%)\"], color=['C0','C1','C2','C3','C4'])\n",
    "plt.title(\"Model Accuracy on Test Set (In-Range vs Out-of-Range)\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.ylim(0, 100)\n",
    "for i, v in enumerate(acc_data[\"Test Accuracy (%)\"]):\n",
    "    plt.text(i, v+1, f\"{v:.1f}%\", ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Export and Reusability\n",
    "\n",
    "Finally, we export the trained models to TensorFlow Lite (TFLite) format so they can be deployed to mobile or edge devices. The saved files are named with the material name and model type (e.g., `MediumCherry_multi_class.tflite`).\n",
    "\n",
    "To adapt this notebook for a different material, simply update the `material_name` and `base_data_dir` at the top. The rest of the pipeline is modular and will automatically process the new dataset (provided it follows the same folder structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b02e586-2c2e-460a-9d8a-404c6d27e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export models to TensorFlow Lite\n",
    "export_dir = \"./tflite_models\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "models_to_export = {\n",
    "    \"multi_class\": model_multi,\n",
    "    \"binary\": model_binary,\n",
    "    \"regression\": model_regression,\n",
    "    \"ordinal\": model_ordinal,\n",
    "    \"multi_task\": model_multi_task\n",
    "}\n",
    "\n",
    "for name, model in models_to_export.items():\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    tflite_path = os.path.join(export_dir, f\"{material_name}_{name}.tflite\")\n",
    "    with open(tflite_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(f\"Saved {name} model to {tflite_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
