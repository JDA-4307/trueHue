{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Material Color Range Validation - Model Comparison\n",
    "\n",
    "This notebook demonstrates how to build and compare five different AI models to determine whether an image of a material is **in-range** or **out-of-range** based on its color/lightness. We will use transfer learning with a pre-trained convolutional neural network (CNN) backbone and evaluate the following approaches:\n",
    "\n",
    "- **Multi-Class Classification** – 5-way classification (in-range: light, standard, dark; out-of-range: too light, too dark).\n",
    "- **Binary Classification** – 2-way classification (in-range vs. out-of-range).\n",
    "- **Regression** – Predict a continuous brightness score and threshold it to classify range.\n",
    "- **Ordinal Regression** – Treat the problem as an ordinal classification, exploiting the ordered nature of brightness levels.\n",
    "- **Hybrid Multi-Task** – A model with two heads: one for 5-class classification and one for regression, sharing a common CNN base.\n",
    "\n",
    "This optimized version trains models sequentially to avoid memory issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, pathlib, gc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, mixed_precision\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Memory monitoring function\n",
    "def print_memory_usage():\n",
    "    \"\"\"Print the current memory usage of the Python process\"\"\"\n",
    "    import psutil\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(f\"Memory usage: {process.memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "# Memory cleanup function\n",
    "def cleanup_memory():\n",
    "    \"\"\"Clean up memory between model training sessions\"\"\"\n",
    "    print(\"Cleaning up memory...\")\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "# Uncomment to use mixed precision (helps with memory usage on compatible GPUs)\n",
    "# mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Optional: restrict to CPU if GPU is causing issues\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define the material dataset directory (change material_name to reuse for a different material)\n",
    "material_name = \"MediumCherry\"\n",
    "base_data_dir = pathlib.Path(\"/Users/rishimanimaran/Documents/College/junior-year/spring-2025/cs-3312/color-validation-app-spring\") / material_name\n",
    "\n",
    "# Create checkpoint directory\n",
    "checkpoint_dir = f\"./checkpoints/{material_name}\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Define class names in order of increasing darkness (brightness decreases from first to last)\n",
    "class_names = [\"out-of-range-too-light\", \"in-range-light\", \"in-range-standard\", \"in-range-dark\", \"out-of-range-too-dark\"]\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the directories exist\n",
    "for cname in class_names:\n",
    "    if not (base_data_dir / cname).exists():\n",
    "        raise FileNotFoundError(f\"Directory not found: {base_data_dir/cname} (please check the path and folder names)\")\n",
    "\n",
    "# Collect all image file paths and their class labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "for idx, cname in enumerate(class_names):\n",
    "    for filepath in (base_data_dir / cname).glob(\"*.*\"):\n",
    "        image_paths.append(str(filepath))\n",
    "        labels.append(idx)\n",
    "image_paths = np.array(image_paths)\n",
    "labels = np.array(labels)\n",
    "num_images = len(labels)\n",
    "print(f\"Found {num_images} images for material '{material_name}'.\")\n",
    "\n",
    "# Shuffle and split into train/validation/test (80/10/10 split)\n",
    "indices = np.arange(num_images)\n",
    "np.random.shuffle(indices)\n",
    "train_end = int(0.8 * num_images)\n",
    "val_end = int(0.9 * num_images)\n",
    "train_indices = indices[:train_end]\n",
    "val_indices   = indices[train_end:val_end]\n",
    "test_indices  = indices[val_end:]\n",
    "train_paths, train_labels = image_paths[train_indices], labels[train_indices]\n",
    "val_paths,   val_labels   = image_paths[val_indices],   labels[val_indices]\n",
    "test_paths,  test_labels  = image_paths[test_indices],  labels[test_indices]\n",
    "print(f\"Split: {len(train_labels)} training, {len(val_labels)} validation, {len(test_labels)} test images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and resize an image from a file path\n",
    "def load_and_resize(image_path, label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_image(image, channels=3, expand_animations=False)  # decode JPEG/PNG/etc.\n",
    "    image = tf.image.resize(image, [224, 224])  # resize to 224x224\n",
    "    image = tf.cast(image, tf.float32)         # convert to float32\n",
    "    return image, label\n",
    "\n",
    "# Create TensorFlow Dataset objects from file paths and labels\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "val_ds   = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "test_ds  = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
    "\n",
    "# Apply the loading function\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_ds = train_ds.map(load_and_resize, num_parallel_calls=AUTOTUNE)\n",
    "val_ds   = val_ds.map(load_and_resize, num_parallel_calls=AUTOTUNE)\n",
    "test_ds  = test_ds.map(load_and_resize, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation pipeline for training images\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),           # random horizontal flip\n",
    "    layers.RandomRotation(0.05),                   # random rotation (±5%)\n",
    "    layers.RandomZoom(0.1),                        # random zoom\n",
    "    # layers.RandomTranslation(...) could be added for shifts\n",
    "])\n",
    "# Note: We avoid color/brightness augmentations, as those would alter the label (brightness range).\n",
    "\n",
    "# Normalize pixel values from [0,255] to [0,1]\n",
    "normalization_layer = layers.Rescaling(1./255)\n",
    "\n",
    "# Apply augmentation (training only) and normalization\n",
    "def preprocess_train(img, lbl):\n",
    "    img = data_augmentation(img, training=True)\n",
    "    img = normalization_layer(img)\n",
    "    return img, lbl\n",
    "\n",
    "def preprocess_eval(img, lbl):\n",
    "    img = normalization_layer(img)\n",
    "    return img, lbl\n",
    "\n",
    "train_ds = train_ds.map(preprocess_train, num_parallel_calls=AUTOTUNE)\n",
    "val_ds   = val_ds.map(preprocess_eval, num_parallel_calls=AUTOTUNE)\n",
    "test_ds  = test_ds.map(preprocess_eval, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Use a smaller batch size to reduce memory usage\n",
    "batch_size = 16\n",
    "train_ds = train_ds.shuffle(buffer_size=1000, seed=42, reshuffle_each_iteration=True)\n",
    "train_ds = train_ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "val_ds   = val_ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "test_ds  = test_ds.batch(batch_size).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define brightness values for each class (for regression target)\n",
    "# These are hypothetical \"brightness scores\" for each category, 0=darkest, 1=brightest\n",
    "brightness_values = tf.constant([0.9, 0.75, 0.5, 0.25, 0.1], dtype=tf.float32)\n",
    "# Define thresholds for in-range vs out-of-range based on the midpoints between categories\n",
    "upper_threshold = (float(brightness_values[0]) + float(brightness_values[1])) / 2.0  # between \"too_light\" and \"in_range_light\"\n",
    "lower_threshold = (float(brightness_values[-2]) + float(brightness_values[-1])) / 2.0  # between \"in_range_dark\" and \"too_dark\"\n",
    "print(f\"Brightness thresholds for 'in-range': lower={lower_threshold:.3f}, upper={upper_threshold:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Functions\n",
    "\n",
    "We define functions to create, train and evaluate each type of model. This allows us to handle one model at a time instead of having all models in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_variants():\n",
    "    \"\"\"Create specialized dataset variants for each model type\"\"\"\n",
    "    # For multi-class classification (one-hot encoded labels)\n",
    "    train_ds_multi = train_ds.map(lambda x, y: (x, tf.one_hot(y, depth=5)), num_parallel_calls=AUTOTUNE)\n",
    "    val_ds_multi = val_ds.map(lambda x, y: (x, tf.one_hot(y, depth=5)), num_parallel_calls=AUTOTUNE)\n",
    "    test_ds_multi = test_ds.map(lambda x, y: (x, tf.one_hot(y, depth=5)), num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    # For binary classification (in-range vs out-of-range)\n",
    "    train_ds_bin = train_ds.map(lambda x, y: (x, tf.cast((y >= 1) & (y <= 3), tf.float32)), num_parallel_calls=AUTOTUNE)\n",
    "    val_ds_bin = val_ds.map(lambda x, y: (x, tf.cast((y >= 1) & (y <= 3), tf.float32)), num_parallel_calls=AUTOTUNE)\n",
    "    test_ds_bin = test_ds.map(lambda x, y: (x, tf.cast((y >= 1) & (y <= 3), tf.float32)), num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    # For regression (brightness score targets)\n",
    "    train_ds_reg = train_ds.map(lambda x, y: (x, tf.gather(brightness_values, tf.cast(y, tf.int32))), num_parallel_calls=AUTOTUNE)\n",
    "    val_ds_reg = val_ds.map(lambda x, y: (x, tf.gather(brightness_values, tf.cast(y, tf.int32))), num_parallel_calls=AUTOTUNE)\n",
    "    test_ds_reg = test_ds.map(lambda x, y: (x, tf.gather(brightness_values, tf.cast(y, tf.int32))), num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    # For ordinal regression (ordered binary targets)\n",
    "    def ordinal_targets(label):\n",
    "        # label is scalar tensor (0-4), compare with [0,1,2,3]\n",
    "        return tf.cast(label > tf.range(4, dtype=tf.int32), tf.float32)\n",
    "    \n",
    "    train_ds_ord = train_ds.map(lambda x, y: (x, ordinal_targets(y)), num_parallel_calls=AUTOTUNE)\n",
    "    val_ds_ord = val_ds.map(lambda x, y: (x, ordinal_targets(y)), num_parallel_calls=AUTOTUNE)\n",
    "    test_ds_ord = test_ds.map(lambda x, y: (x, ordinal_targets(y)), num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    # For multi-task (classification + regression)\n",
    "    train_ds_mt = train_ds.map(lambda x, y: (x, (tf.one_hot(y, depth=5), tf.gather(brightness_values, tf.cast(y, tf.int32)))), num_parallel_calls=AUTOTUNE)\n",
    "    val_ds_mt = val_ds.map(lambda x, y: (x, (tf.one_hot(y, depth=5), tf.gather(brightness_values, tf.cast(y, tf.int32)))), num_parallel_calls=AUTOTUNE)\n",
    "    test_ds_mt = test_ds.map(lambda x, y: (x, (tf.one_hot(y, depth=5), tf.gather(brightness_values, tf.cast(y, tf.int32)))), num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    return {\n",
    "        'multi_class': (train_ds_multi, val_ds_multi, test_ds_multi),\n",
    "        'binary': (train_ds_bin, val_ds_bin, test_ds_bin),\n",
    "        'regression': (train_ds_reg, val_ds_reg, test_ds_reg),\n",
    "        'ordinal': (train_ds_ord, val_ds_ord, test_ds_ord),\n",
    "        'multi_task': (train_ds_mt, val_ds_mt, test_ds_mt)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multi_class_model(dataset_variants):\n",
    "    \"\"\"Train the Multi-Class Classification model\"\"\"\n",
    "    print(\"\\n===== Training Multi-Class Model =====\")\n",
    "    print_memory_usage()\n",
    "    \n",
    "    train_ds_multi, val_ds_multi, test_ds_multi = dataset_variants['multi_class']\n",
    "    \n",
    "    # Create a checkpoint callback\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, \"multi_class_best.h5\"),\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Create the model\n",
    "    base_model = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False, \n",
    "        weights=\"imagenet\", \n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    base_model.trainable = False # Freeze base model layers initially\n",
    "    \n",
    "    inputs = keras.Input(shape=(224, 224, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.2)(x)  # dropout for regularization\n",
    "    output_multi = layers.Dense(5, activation=\"softmax\")(x)  # 5 classes softmax\n",
    "    \n",
    "    model = keras.Model(inputs, output_multi, name=\"MultiClassModel\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"categorical_crossentropy\", \n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    model.summary()\n",
    "    \n",
    "    # Initial training with frozen base\n",
    "    epochs_initial = 5\n",
    "    print(\"\\nPhase 1: Training with frozen base model\")\n",
    "    history = model.fit(\n",
    "        train_ds_multi, \n",
    "        validation_data=val_ds_multi,\n",
    "        epochs=epochs_initial,\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "    \n",
    "    # Fine-tuning phase - unfreeze some layers\n",
    "    print(\"\\nPhase 2: Fine-tuning\")\n",
    "    base_model.trainable = True\n",
    "    # Freeze all layers except the last 20 layers in the base model\n",
    "    for layer in base_model.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Recompile with a lower learning rate for fine-tuning\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "        loss=\"categorical_crossentropy\", \n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    # Continue training\n",
    "    epochs_finetune = 5\n",
    "    history_ft = model.fit(\n",
    "        train_ds_multi, \n",
    "        validation_data=val_ds_multi,\n",
    "        epochs=epochs_initial + epochs_finetune, \n",
    "        initial_epoch=history.epoch[-1] + 1,\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "    \n",
    "    # Plot training curves\n",
    "    acc = history.history['accuracy'] + history_ft.history.get('accuracy', [])\n",
    "    val_acc = history.history['val_accuracy'] + history_ft.history.get('val_accuracy', [])\n",
    "    loss = history.history['loss'] + history_ft.history.get('loss', [])\n",
    "    val_loss = history.history['val_loss'] + history_ft.history.get('val_loss', [])\n",
    "    \n",
    "    epochs_range = range(1, len(acc) + 1)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs_range, loss, label='Train Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Val Loss')\n",
    "    plt.title('Multi-Class Model Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs_range, acc, label='Train Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Val Accuracy')\n",
    "    plt.title('Multi-Class Model Accuracy')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{material_name}_multi_class_training.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set:\")\n",
    "    test_loss, test_acc = model.evaluate(test_ds_multi, verbose=1)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_save_path = f\"{material_name}_multi_class_model\"\n",
    "    model.save(model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "    \n",
    "    # Convert to TFLite\n",
    "    export_dir = \"./tflite_models\"\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    tflite_path = os.path.join(export_dir, f\"{material_name}_multi_class.tflite\")\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    with open(tflite_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "        \n",
    "    print(f\"TFLite model saved to {tflite_path}\")\n",
    "    \n",
    "    # Get predictions for accuracy calculation\n",
    "    y_pred_proba = model.predict(test_ds_multi)\n",
    "    y_pred_class = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    # We need to extract the original test labels for comparison\n",
    "    y_true = []\n",
    "    for _, labels in test_ds.unbatch():\n",
    "        y_true.append(labels.numpy())\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    # Calculate multi-class accuracy\n",
    "    multi_class_accuracy = np.mean(y_pred_class == y_true)\n",
    "    \n",
    "    # Convert to binary (in-range vs out-of-range)\n",
    "    y_pred_inrange = np.isin(y_pred_class, [1,2,3]).astype(int)\n",
    "    y_true_inrange = np.isin(y_true, [1,2,3]).astype(int)\n",
    "    binary_accuracy = np.mean(y_pred_inrange == y_true_inrange)\n",
    "    \n",
    "    print(f\"Test 5-class accuracy: {multi_class_accuracy:.4f}\")\n",
    "    print(f\"Test binary accuracy (in-range vs out-of-range): {binary_accuracy:.4f}\")\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del model\n",
    "    del base_model\n",
    "    cleanup_memory()\n",
    "    \n",
    "    return multi_class_accuracy, binary_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binary_model(dataset_variants):\n",
    "    \"\"\"Train the Binary Classification model\"\"\"\n",
    "    print(\"\\n===== Training Binary Classification Model =====\")\n",
    "    print_memory_usage()\n",
    "    \n",
    "    train_ds_bin, val_ds_bin, test_ds_bin = dataset_variants['binary']\n",
    "    \n",
    "    # Create a checkpoint callback\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, \"binary_best.h5\"),\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Create the model\n",
    "    base_model = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False, \n",
    "        weights=\"imagenet\", \n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    inputs = keras.Input(shape=(224, 224, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    output_bin = layers.Dense(1, activation=\"sigmoid\")(x)  # single sigmoid output\n",
    "    \n",
    "    model = keras.Model(inputs, output_bin, name=\"BinaryModel\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"binary_crossentropy\", \n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    model.summary()\n",
    "    \n",
    "    # Initial training with frozen base\n",
    "    epochs_initial = 5\n",
    "    print(\"\\nPhase 1: Training with frozen base model\")\n",
    "    history = model.fit(\n",
    "        train_ds_bin, \n",
    "        validation_data=val_ds_bin,\n",
    "        epochs=epochs_initial,\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "    \n",
    "    # Fine-tuning phase\n",
    "    print(\"\\nPhase 2: Fine-tuning\")\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "        loss=\"binary_crossentropy\", \n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    epochs_finetune = 5\n",
    "    history_ft = model.fit(\n",
    "        train_ds_bin, \n",
    "        validation_data=val_ds_bin,\n",
    "        epochs=epochs_initial + epochs_finetune, \n",
    "        initial_epoch=history.epoch[-1] + 1,\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "    \n",
    "    # Plot training curves\n",
    "    acc = history.history['accuracy'] + history_ft.history.get('accuracy', [])\n",
    "    val_acc = history.history['val_accuracy'] + history_ft.history.get('val_accuracy', [])\n",
    "    loss = history.history['loss'] + history_ft.history.get('loss', [])\n",
    "    val_loss = history.history['val_loss'] + history_ft.history.get('val_loss', [])\n",
    "    \n",
    "    epochs_range = range(1, len(acc) + 1)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs_range, loss, label='Train Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Val Loss')\n",
    "    plt.title('Binary Model Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs_range, acc, label='Train Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Val Accuracy')\n",
    "    plt.title('Binary Model Accuracy')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{material_name}_binary_training.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set:\")\n",
    "    test_loss, test_acc = model.evaluate(test_ds_bin, verbose=1)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_save_path = f\"{material_name}_binary_model\"\n",
    "    model.save(model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "    \n",
    "    # Convert to TFLite\n",
    "    export_dir = \"./tflite_models\"\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    tflite_path = os.path.join(export_dir, f\"{material_name}_binary.tflite\")\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    with open(tflite_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "        \n",
    "    print(f\"TFLite model saved to {tflite_path}\")\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del model\n",
    "    del base_model\n",
    "    cleanup_memory()\n",
    "    \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression_model(dataset_variants):\n",
    "    \"\"\"Train the Regression model\"\"\"\n",
    "    print(\"\\n===== Training Regression Model =====\")\n",
    "    print_memory_usage()\n",
    "    \n",
    "    train_ds_reg, val_ds_reg, test_ds_reg = dataset_variants['regression']\n",
    "    \n",
    "    # Create a checkpoint callback\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, \"regression_best.h5\"),\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Create the model\n",
    "    base_model = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False, \n",
    "        weights=\"imagenet\", \n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    inputs = keras.Input(shape=(224, 224, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    output_reg = layers.Dense(1, activation=\"linear\")(x)  # linear output for brightness score\n",
    "    \n",
    "    model = keras.Model(inputs, output_reg, name=\"RegressionModel\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"mse\"\n",
    "    )\n",
    "    model.summary()\n",
    "    \n",
    "    # Initial training with frozen base\n",
    "    epochs_initial = 5\n",
    "    print(\"\\nPhase 1: Training with frozen base model\")\n",
    "    history = model.fit(\n",
    "        train_ds_reg, \n",
    "        validation_data=val_ds_reg,\n",
    "        epochs=epochs_initial,\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "    \n",
    "    # Fine-tuning phase\n",
    "    print(\"\\nPhase 2: Fine-tuning\")\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "        loss=\"mse\"\n",
    "    )\n",
    "    \n",
    "    epochs_finetune = 5\n",
    "    history_ft = model.fit(\n",
    "        train_ds_reg, \n",
    "        validation_data=val_ds_reg,\n",
    "        epochs=epochs_initial + epochs_finetune, \n",
    "        initial_epoch=history.epoch[-1] + 1,\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "    \n",
    "    # Plot training curves\n",
    "    loss = history.history['loss'] + history_ft.history.get('loss', [])\n",
    "    val_loss = history.history['val_loss'] + history_ft.history.get('val_loss', [])\n",
    "    \n",
    "    epochs_range = range(1, len(loss) + 1)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    \n",
    "    plt.plot(epochs_range, loss, label='Train MSE')\n",
    "    plt.plot(epochs_range, val_loss, label='Val MSE')\n",
    "    plt.title('Regression Model Training (MSE)')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Mean Squared Error'); plt.legend()\n",
    "    plt.savefig(f\"{material_name}_regression_training.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set:\")\n",
    "    test_loss = model.evaluate(test_ds_reg, verbose=1)\n",
    "    print(f\"Test MSE: {test_loss:.4f}\")\n",
    "    \n",
    "    # Make predictions for binary accuracy calculation\n",
    "    y_pred_reg = model.predict(test_ds_reg).ravel()\n",
    "    \n",
    "    # Classify as in-range (1) if between lower_threshold and upper_threshold, else out-of-range (0)\n",
    "    y_pred_inrange = np.where((y_pred_reg >= lower_threshold) & (y_pred_reg <= upper_threshold), 1, 0)\n",
    "    \n",
    "    # Get true labels\n",
    "    y_true = []\n",
    "    for _, labels in test_ds.unbatch():\n",
    "        y_true.append(labels.numpy())\n",
    "    y_true = np.array(y_true)\n",
    "    y_true_inrange = np.isin(y_true, [1,2,3]).astype(int)\n",
    "    \n",
    "    # Calculate binary accuracy\n",
    "    binary_accuracy = np.mean(y_pred_inrange == y_true_inrange)\n",
    "    print(f\"Binary accuracy (in-range vs out-of-range): {binary_accuracy:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_save_path = f\"{material_name}_regression_model\"\n",
    "    model.save(model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "    \n",
    "    # Convert to TFLite\n",
    "    export_dir = \"./tflite_models\"\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    tflite_path = os.path.join(export_dir, f\"{material_name}_regression.tflite\")\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    with open(tflite_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "        \n",
    "    print(f\"TFLite model saved to {tflite_path}\")\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del model\n",
    "    del base_model\n",
    "    cleanup_memory()\n",
    "    \n",
    "    return binary_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ordinal_model(dataset_variants):\n",
    "    \"\"\"Train the Ordinal Regression model\"\"\"\n",
    "    print(\"\\n===== Training Ordinal Regression Model =====\")\n",
    "    print_memory_usage()\n",
    "    \n",
    "    train_ds_ord, val_ds_ord, test_ds_ord = dataset_variants['ordinal']\n",
    "    \n",
    "    # Create a checkpoint callback\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, \"ordinal_best.h5\"),\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Create the model\n",
    "    base_model = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False, \n",
    "        weights=\"imagenet\", \n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    inputs = keras.Input(shape=(224, 224, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    output_ord = layers.Dense(4, activation=\"sigmoid\")(x)  # 4 sigmoid outputs for ordinal thresholds\n",
    "    \n",
    "    model = keras.Model(inputs, output_ord, name=\"OrdinalModel\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"binary_crossentropy\"\n",
    "    )\n",
    "    model.summary()\n",
    "    \n",
    "    # Initial training with frozen base\n",
    "    epochs_initial = 5\n",
    "    print(\"\\nPhase 1: Training with frozen base model\")\n",
    "    history = model.fit(\n",
    "        train_ds_ord, \n",
    "        validation_data=val_ds_ord,\n",
    "        epochs=epochs_initial,\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "    \n",
    "    # Fine-tuning phase\n",
    "    print(\"\\nPhase 2: Fine-tuning\")\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "        loss=\"binary_crossentropy\"\n",
    "    )\n",
    "    \n",
    "    epochs_finetune = 5\n",
    "    history_ft = model.fit(\n",
    "        train_ds_ord, \n",
    "        validation_data=val_ds_ord,\n",
    "        epochs=epochs_initial + epochs_finetune, \n",
    "        initial_epoch=history.epoch[-1] + 1,\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "    \n",
    "    # Plot training curves\n",
    "    loss = history.history['loss'] + history_ft.history.get('loss', [])\n",
    "    val_loss = history.history['val_loss'] + history_ft.history.get('val_loss', [])\n",
    "    \n",
    "    epochs_range = range(1, len(loss) + 1)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    \n",
    "    plt.plot(epochs_range, loss, label='Train Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Val Loss')\n",
    "    plt.title('Ordinal Model Training Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Binary Crossentropy Loss'); plt.legend()\n",
    "    plt.savefig(f\"{material_name}_ordinal_training.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set:\")\n",
    "    test_loss = model.evaluate(test_ds_ord, verbose=1)\n",
    "    print(f\"Test loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Make predictions for accuracy calculations\n",
    "    y_pred_ord = model.predict(test_ds_ord)\n",
    "    \n",
    "    # Decode ordinal predictions to class labels\n",
    "    y_pred_ord_class = []\n",
    "    for pred in y_pred_ord:\n",
    "        ord_class = 4  # default to last class\n",
    "        for k, p in enumerate(pred):\n",
    "            if p < 0.5:\n",
    "                ord_class = k\n",
    "                break\n",
    "        y_pred_ord_class.append(ord_class)\n",
    "    y_pred_ord_class = np.array(y_pred_ord_class, dtype=int)\n",
    "    \n",
    "    # Get true labels\n",
    "    y_true = []\n",
    "    for _, labels in test_ds.unbatch():\n",
    "        y_true.append(labels.numpy())\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    # Calculate multi-class accuracy\n",
    "    ordinal_class_accuracy = np.mean(y_pred_ord_class == y_true)\n",
    "    print(f\"5-class accuracy: {ordinal_class_accuracy:.4f}\")\n",
    "    \n",
    "    # Convert to binary in-range\n",
    "    y_pred_inrange = np.isin(y_pred_ord_class, [1,2,3]).astype(int)\n",
    "    y_true_inrange = np.isin(y_true, [1,2,3]).astype(int)\n",
    "    binary_accuracy = np.mean(y_pred_inrange == y_true_inrange)\n",
    "    print(f\"Binary accuracy (in-range vs out-of-range): {binary_accuracy:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_save_path = f\"{material_name}_ordinal_model\"\n",
    "    model.save(model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "    \n",
    "    # Convert to TFLite\n",
    "    export_dir = \"./tflite_models\"\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    tflite_path = os.path.join(export_dir, f\"{material_name}_ordinal.tflite\")\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    with open(tflite_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "        \n",
    "    print(f\"TFLite model saved to {tflite_path}\")\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del model\n",
    "    del base_model\n",
    "    cleanup_memory()\n",
    "    \n",
    "    return ordinal_class_accuracy, binary_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multi_task_model(dataset_variants):\n",
    "    \"\"\"Train the Hybrid Multi-Task model\"\"\"\n",
    "    print(\"\\n===== Training Multi-Task Model =====\")\n",
    "    print_memory_usage()\n",
    "    \n",
    "    train_ds_mt, val_ds_mt, test_ds_mt = dataset_variants['multi_task']\n",
    "    \n",
    "    # Create a checkpoint callback\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, \"multi_task_best.h5\"),\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_class_output_accuracy\",\n",
    "        mode=\"max\",\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Create the model\n",
    "    base_model = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False, \n",
    "        weights=\"imagenet\", \n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    inputs = keras.Input(shape=(224, 224, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Classification head (5 classes)\n",
    "    class_output = layers.Dense(5, activation=\"softmax\", name=\"class_output\")(x)\n",
    "    # Regression head (brightness)\n",
    "    reg_output = layers.Dense(1, activation=\"linear\", name=\"reg_output\")(x)\n",
    "    \n",
    "    model = keras.Model(inputs, [class_output, reg_output], name=\"MultiTaskModel\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss={\"class_output\": \"categorical_crossentropy\", \"reg_output\": \"mse\"},\n",
    "        metrics={\"class_output\": \"accuracy\"}\n",
    "    )\n",
    "    model.summary()\n",
    "    \n",
    "    # Initial training with frozen base\n",
    "    epochs_initial = 5\n",
    "    print(\"\\nPhase 1: Training with frozen base model\")\n",
    "    history = model.fit(\n",
    "        train_ds_mt, \n",
    "        validation_data=val_ds_mt,\n",
    "        epochs=epochs_initial,\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "    \n",
    "    # Fine-tuning phase\n",
    "    print(\"\\nPhase 2: Fine-tuning\")\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "        loss={\"class_output\": \"categorical_crossentropy\", \"reg_output\": \"mse\"},\n",
    "        metrics={\"class_output\": \"accuracy\"}\n",
    "    )\n",
    "    \n",
    "    epochs_finetune = 5\n",
    "    history_ft = model.fit(\n",
    "        train_ds_mt, \n",
    "        validation_data=val_ds_mt,\n",
    "        epochs=epochs_initial + epochs_finetune, \n",
    "        initial_epoch=history.epoch[-1] + 1,\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "    \n",
    "    # Plot training curves\n",
    "    acc = history.history['class_output_accuracy'] + history_ft.history.get('class_output_accuracy', [])\n",
    "    val_acc = history.history['val_class_output_accuracy'] + history_ft.history.get('val_class_output_accuracy', [])\n",
    "    loss = history.history['loss'] + history_ft.history.get('loss', [])\n",
    "    val_loss = history.history['val_loss'] + history_ft.history.get('val_loss', [])\n",
    "    \n",
    "    epochs_range = range(1, len(acc) + 1)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs_range, loss, label='Train Total Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Val Total Loss')\n",
    "    plt.title('Multi-Task Model Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs_range, acc, label='Train Class Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Val Class Accuracy')\n",
    "    plt.title('Multi-Task Model Classification Accuracy')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{material_name}_multi_task_training.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set:\")\n",
    "    test_results = model.evaluate(test_ds_mt, verbose=1)\n",
    "    print(f\"Test classification accuracy: {test_results[3]:.4f}\")  # Index 3 is class_output_accuracy\n",
    "    \n",
    "    # Make predictions for accuracy calculations\n",
    "    y_pred_mt = model.predict(test_ds_mt)\n",
    "    y_pred_class_proba = y_pred_mt[0]  # First output is classification probabilities\n",
    "    y_pred_class = np.argmax(y_pred_class_proba, axis=1)\n",
    "    \n",
    "    # Get true labels\n",
    "    y_true = []\n",
    "    for _, labels in test_ds.unbatch():\n",
    "        y_true.append(labels.numpy())\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    # Calculate multi-class accuracy\n",
    "    multi_task_class_accuracy = np.mean(y_pred_class == y_true)\n",
    "    print(f\"5-class accuracy: {multi_task_class_accuracy:.4f}\")\n",
    "    \n",
    "    # Convert to binary in-range\n",
    "    y_pred_inrange = np.isin(y_pred_class, [1,2,3]).astype(int)\n",
    "    y_true_inrange = np.isin(y_true, [1,2,3]).astype(int)\n",
    "    binary_accuracy = np.mean(y_pred_inrange == y_true_inrange)\n",
    "    print(f\"Binary accuracy (in-range vs out-of-range): {binary_accuracy:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_save_path = f\"{material_name}_multi_task_model\"\n",
    "    model.save(model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "    \n",
    "    # Convert to TFLite\n",
    "    export_dir = \"./tflite_models\"\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    tflite_path = os.path.join(export_dir, f\"{material_name}_multi_task.tflite\")\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    with open(tflite_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "        \n",
    "    print(f\"TFLite model saved to {tflite_path}\")\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del model\n",
    "    del base_model\n",
    "    cleanup_memory()\n",
    "    \n",
    "    return multi_task_class_accuracy, binary_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training and Evaluation\n",
    "\n",
    "Now we'll train each model sequentially, tracking results for final comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset variants for each model type\n",
    "dataset_variants = create_dataset_variants()\n",
    "\n",
    "# Train and evaluate models sequentially, collecting results\n",
    "results = {}\n",
    "\n",
    "# Multi-class model\n",
    "multi_class_accuracy, multi_binary_accuracy = train_multi_class_model(dataset_variants)\n",
    "results[\"Multi-Class\"] = multi_binary_accuracy * 100\n",
    "\n",
    "# Binary model\n",
    "binary_accuracy = train_binary_model(dataset_variants)\n",
    "results[\"Binary\"] = binary_accuracy * 100\n",
    "\n",
    "# Regression model\n",
    "reg_binary_accuracy = train_regression_model(dataset_variants)\n",
    "results[\"Regression\"] = reg_binary_accuracy * 100\n",
    "\n",
    "# Ordinal model\n",
    "ordinal_class_accuracy, ord_binary_accuracy = train_ordinal_model(dataset_variants)\n",
    "results[\"Ordinal\"] = ord_binary_accuracy * 100\n",
    "\n",
    "# Multi-task model\n",
    "multi_task_class_accuracy, mt_binary_accuracy = train_multi_task_model(dataset_variants)\n",
    "results[\"Multi-Task\"] = mt_binary_accuracy * 100\n",
    "\n",
    "print(\"\\nTraining and evaluation complete!\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Comparison\n",
    "\n",
    "Now let's compare the performance of all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame and visualize\n",
    "acc_data = {\n",
    "    \"Model\": list(results.keys()),\n",
    "    \"Test Accuracy (%)\":  list(results.values())\n",
    "}\n",
    "acc_df = pd.DataFrame(acc_data)\n",
    "print(\"\\nTest Accuracy (In-Range vs Out-of-Range):\")\n",
    "display(acc_df)\n",
    "\n",
    "# Bar chart of accuracies\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(acc_data[\"Model\"], acc_data[\"Test Accuracy (%)\"], color=['C0','C1','C2','C3','C4'])\n",
    "plt.title(f\"Model Accuracy on Test Set for {material_name} (In-Range vs Out-of-Range)\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.ylim(0, 105)  # Leave room for text\n",
    "for i, v in enumerate(acc_data[\"Test Accuracy (%)\"]):\n",
    "    plt.text(i, v+1, f\"{v:.1f}%\", ha='center')\n",
    "plt.savefig(f\"{material_name}_model_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "This notebook has demonstrated how to build and compare five different models for material color range validation. By training models sequentially and implementing memory optimization techniques, we've avoided kernel crashes while maintaining model performance.\n",
    "\n",
    "To adapt this notebook for a different material:\n",
    "1. Update the `material_name` and `base_data_dir` variables\n",
    "2. Ensure the folder structure follows the same pattern with 5 categories\n",
    "3. Run the notebook to train and compare models\n",
    "\n",
    "All models have been saved in TensorFlow and TFLite formats for deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
